## 激活函数
### 1. Sigmoid 激活函数
- 优点：曲线平滑，导数容易计算
- 缺点：计算耗时，梯度进入饱和区容易造成梯度消失难以收敛
### 2. Softmax 激活函数
- 处理多分类问题，输出每一个类别对应的输出的概率，概率最大者为预测结果
### 3. ReLU(Rectified linear unit) 激活函数
- 分段线性，单侧抑制
- 模型收敛快，梯度不会消失
- 稀疏激活神经网络
- 变化形式：LeakyReLU,eReLU,PReLu等

## 优化器
### 1. SGD(Stochastic Gradient Descent)随机梯度下降
- 从梯度下降演变而来
  + 随机打乱全部数据
  + 重复执行梯度下降，但是是每次只随机抽取一个样本代入计算
  + 学习率对训练结果影响很大，过大或过小的学习率都不利于梯度下降

### 2. BGD（Bath Gradient Descent）批量梯度下降
- 从梯度下降演变而来
  + 随机打乱全部数据
  + 每次随机选一小批样本代入计算

### 3. 自动调节学习率的优化器
- 能够根据在训练过程中自适应调节学习率
- Adadelta优化器
- Adagrad 优化器，等

## 损失函数-用于衡量模型一次预测结果的准确程度
### 1. 均方误差
- 平方损失函数是预测值与真实值之间的偏差的平方
- 对多个样本求平方损失的平均值即为均方误差
- 改平方为取绝对值为平均绝对误差
- 对均方误差开根号则为均方根误差

### 2. 交叉熵损失函数
- 发生概率越小的事情一旦发生则其信息量就越大
- 信息量I(x)=log(1/P(x))=-log(P(x))
- 交叉熵H(p,q)=-p(x)log(q(x))

$H(p,q)=\frac{}$




# 第五章 卷积神经网络(CNN)
>![CNN struction](image/conv_stru.png)

---
## 1. 基本概念

### 1.1 卷积
**卷积核**可以看作是对对应区域内的加权求和，对应局部感知，卷积核区域大小叫感受野。
  - 窄卷积,输出n-m+1
  - 宽卷积,输出n+m-1
  - 等宽卷积,输出n

**二维卷积过程**如下，假设卷积层的输入神经元个数为n，卷积核大小为m，步长（stride）为s，
输入 神经元两端各填补p个零（zero padding），那么该卷积后输出的神经元数量为(n−m+2p)/s+1。

>![conv](image/conv.gif)

  - 其他卷积形式
    * 转置卷积（反卷积），从低维度向高维度变的
    * 微步卷积，向低维度数据中间插入0，然后再卷积，可以提高数据维度
    * 空洞卷积（膨胀卷积），向卷积核中插入空洞，变相增加卷积核大小，提高了感受野，但是没有增加参数量

### 1.2 互相关
- 与卷积的区别是卷积核不翻转

---
## 2. 卷积神经网络-前馈神经网络，误差反向传播

### 2.1 卷积层
- 卷积核
  - 决定感受野大小
  - 提取局部区域的特征
  - 在同一层使用不同卷积核可以提取到多个不同特征

- Feature Map
  - 经卷积核提取后的特征图

### 2.2 汇聚层/池化层
- 进行特征选择，减少参数数量
  - Maximun Pooling 取采样区域内的最大值作为该区域的值
  - Mean pooling 取采样区域内的平均值作为该区域的值
  >![Max pooling](image/max_pooling.png)

### 2.3 全连接层
### 2.4 BN层（Bath Normalization）
- 标准化向量层，在神经网络训练过程中，使得每一层神经网络的输入保持相同分布。

### 2.5 droup out 层
- 随机抛弃一些全连接网络中的神经元，使得每一个批次都训练不同的网络，减少过拟合的发生，也精简网络结构

### 2.6 flatten层
- 将二维以上数据展平为一维向量数列，方便与全连接层神经元一一对应。


---
## 3. 结构特性
  - 局部连接
    上一层神经元只与卷积核窗口内神经元相连
  - 权重共享
    卷积核对一层所有神经元权重共享
  - 汇聚

---
## 4. 参数学习
### 4.1 卷积核权重
- 计算损失函数对权重的梯度

### 4.2 卷积核偏置
- 计算损失函数对偏置的梯度

### 4.3 误差项计算
- 汇聚层/池化层
  * 最大池化
      + 误差项回传时，传到上一层池化区域内最大值对应的神经元，其他神经元误差为0
  * 平均池化
      + 误差项回传时，平均传到上一层池化区域内，所有神经元误差相同
- 卷积层
>![Eq 5.32](image/eq5.32.png)
>![Eq 5.36](image/eq5.36.png)

## 5. 几种典型的CNN
  - **LeNet-5**
>![LeNet5](image/LeNet-5_1.png)
>![LeNet5](image/LeNet-5_2.png)

  - **AlexNet**
    * 首次采用GPU训练
    * 采用ReLU激活
    * 使用Dropout 防止过拟合
    >![AlexNet](image/AlexNet.png)
    >![AlexNet](image/AlexNet2.png)
    注：图5.12中最左边224应该为227
  - **Inception网络**
      * 由Inception 模块和少量池化层堆叠而成
      * GoogleNet 有9个Iception模块，5个池化层和其他卷积层和全连接层组成
      * Inception v3把大卷积核换成小卷积核
      * **1x1卷积核**的作用：
        + 当卷积核数量比输入的通道数少(多)时，能起到降维(升维)的作用
        + 其降维或升维的本质是在通道间将信息线性组合变化，增加通道间的信息交互
        + 在保持feature map尺寸不变的情况下，大幅增加非线性特性，可以把网络做得更深
      >![Inception](image/Inception.png)

  - **ResNet 残差网络**
      * 把目标函数拆分为恒等函数和残差函数，让神经网络去学习残差函数，比直接学习目标函数更容易
      >![ResNet](image/ResNet-1.png)
      >![ResNet](image/ResNet-2.png)

(第五章完结)

# 第六章 循环神经网络（RNN）
一类具有短期记忆能力的神经网络,长于处理时序数据并利用其历史信息。

## 1. 给神经网络增加短期记忆能力的方法：
  - 延时神经网络（Time Delay NN）
    * 建立额外的延时单元，记录最近几次神经元的输出，使得下一层神经元的输入为上一层神经元最近多次的输出而不仅仅是一次。
  - 自回归模型(Auroregressive Model)
    * 用神经元的历史信息来预测自己，让历史信息能够影响现在
  - 循环神经网络（RNN）

    >![RNN](image/RNN.png)

## 2. 简单循环网络-只有一个隐藏层的神经网络
>![srn](image/srn-1.png)
>![srn](image/srn-2.png)

### 循环神经网络的计算能力
- ####  通用近似定理
如果一个完全连接的循环神经网络有足够数量的sigmoid型隐藏神经元，它可以以任意准确率去近似任何一个非线性动力系统。
- ####  图灵完备
所有的图灵机都可以被一个由使用Sigmoid 型激活函数的神经元构成的全连接循环网络来进行模拟。

- ####  因此，一个完全连接的循环神经网络可以**近似解决所有的**可计算问题。

## 3. 应用到机器学习
### 3.1 序列到类别模式
输入序列，输出为类别，即分类模式，还可以按时间平均采样
>![s-c](image/s-c.png)

### 3.2 同步的序列到序列模式
主要用于序列标注任务,如词性标注，即每一时刻都有输入和输出，输入序列和输出序列的长度相同。
>![s-c](image/s-s.png)

### 3.3 异步的序列到序列模式
也称为编码器-解码器（Encoder-Decoder）模型，需要先编码后解码，
输入序列和输出序列不需要有严格的对应关系，也不需要保持相同的长度。
比如在机器翻译中，输入为源语言的单词序列，输出为目标语言的单词序列。
>![s-c](image/s-s-y.png)

## 4. 参数学习-梯度下降
### 4.1 随时间反向传播（Backpropagation Through Time）
  * 把循环神经网络看作展开的多层神经网络，每一个时刻对应一层神经网络
  * 所有层参数共享，因此真实梯度是所有展开层的参数梯度之和
  * 参数梯度需要"向前"计算和"反向"计算后才能得到参数更新

### 4.2 实时循环学习（RTRL）
（未完待续。。）


# 总结
## 1. 机器学习的通用工作流程
  * 定义问题
    + 预测目标
    + 可用的数据
    + 是否有标注
  *  评估预测是否成功的标准
  *  评估模型的验证过程
  *  数据向量化
  *  开发第一个模型
    + 要打败基于常识的简单基准方法，证明机器学习的方法有效
  * 通过调节超参数和添加正则化来逐步改进模型架构
    + 根据模型在验证集上的性能来调参，不是测试集或训练集
    + 先让模型过拟合再开始添加正则化或降低模型尺寸
  * 调节超参数要防止验证集过拟合
    + 因此要保存一个测试集，最终检验模型效果


## 2. 三种主要网络架构：Dens，CNN，RNN
  * 向量数据：Dens
  * 图像数据：CNN-2D
  * 声音数据：CNN-1D或RNN
  * 文本数据：CNN-1D或RNN
  * 时间序列数据：RNN或CNN-1D
  * 其他序列数据：RNN或CNN-1D
  * 视频数据：CNN-3D或帧级CNN-2D+RNN/CNN-1D
  * 立体数据：CNN-3D

### 2.1 密集连接网络
由Dense层堆叠而成，层中每个单元都与其他所有单元想连接，主要用于处理向量数据，常用于分类数据，还用于大多数网络最终分类或者回归的阶段。
  * 对于二分类问题，最后一个Dense层只有一个单元，使用sigmoid激活函数，并采用binary_crossentropy作为损失函数，目标是0或1
  * 对于单标签多分类问题，每个样本只有一个类别，最后Dense层单元个数等于分类数，使用softmax激活
    + 如果目标是one-hot编码，采用categorical_crossentropy作为损失
    + 如果目标是整数，则用sparse_categorical_crossentropy作为损失
  * 对于多标签多分类问题，每个样本可能对应多个类别，最后Dense层单元个数等于分类数，使用sigmoid激活，并采用binary_crossentropy作为损失，目标应该是k-hot编码的
  * 对于连续值向量的回归问题，最后是一层不带激活的Dense层，单元个数等于要预测的值的个数（通常只有一个值，比如房价），损失函数有
    + mean_squared_error(MSE)
    + mean_absolute_error(MAE)


### 2.2 卷积神经网络
卷积层能够查看空间局部模式，其方法是对输入张量的不同空间位置应用相同的几何变换，这样得到的表示具有平移不变性，使得卷积层能够高效利用数据，并且高度模块化。这种方法适用于一维（序列）、二维（图像）和三维（立体数据）等。
卷积神经网络是卷积层和最大池化层的堆叠，最后一层通常是Flatten或全局池化层，然后再是Densen层。池化层对数据进行空间下采样，可以达到两个目的
* 让特征图尺寸保持在合理的范围内
* 让后面的卷积层能够看到输入中更大的空间范围

普通卷积层很可能会被深度可分离卷积层所取代，与普通卷积层相比，深度可分离卷积层效果一致，但是速度更快，表示效率更高。

### 2.3 循环神经网络（RNN，GRU，LSTM）
RNN的工作原理是对输入序列每次处理一个时间步，并且始终保存一个状态。如果序列中的模式具有时间平移不变性，即最近的过去比遥远的过去更重要，应该优先采用RNN而不是CNN-1D。
