# 第五章 卷积神经网络(CNN)
>![CNN struction](image/conv_stru.png)   

---
## 1. 基本概念

### 1.1 卷积
**卷积核**可以看作是对对应区域内的加权求和，对应局部感知，卷积核区域大小叫感受野。
  - 窄卷积,输出n-m+1
  - 宽卷积,输出n+m-1
  - 等宽卷积,输出n   

**二维卷积过程**如下，假设卷积层的输入神经元个数为n，卷积核大小为m，步长（stride）为s，   
输入 神经元两端各填补p个零（zero padding），那么该卷积后输出的神经元数量为(n−m+2p)/s+1。   

>![conv](image/conv.gif)   

  - 其他卷积形式
    * 转置卷积（反卷积），从低维度向高维度变的
    * 微步卷积，向低维度数据中间插入0，然后再卷积，可以提高数据维度
    * 空洞卷积（膨胀卷积），向卷积核中插入空洞，变相增加卷积核大小，提高了感受野，但是没有增加参数量

### 1.2 互相关
- 与卷积的区别是卷积核不翻转   

---
## 2. 卷积神经网络-前馈神经网络，误差反向传播   

### 2.1 卷积层
- 卷积核
  - 决定感受野大小
  - 提取局部区域的特征
  - 在同一层使用不同卷积核可以提取到多个不同特征   

- Feature Map
  - 经卷积核提取后的特征图

### 2.2 汇聚层/池化层
- 进行特征选择，减少参数数量
  - Maximun Pooling 取采样区域内的最大值作为该区域的值
  - Mean pooling 取采样区域内的平均值作为该区域的值   
  >![Max pooling](image/max_pooling.png)   

### 2.3 全连接层   

---
## 3. 结构特性
  - 局部连接   
    上一层神经元只与卷积核窗口内神经元相连
  - 权重共享   
    卷积核对一层所有神经元权重共享
  - 汇聚   

---
## 4. 参数学习
### 4.1 卷积核权重
- 计算损失函数对权重的梯度   

### 4.2 卷积核偏置
- 计算损失函数对偏置的梯度   

### 4.3 误差项计算
- 汇聚层/池化层
  * 最大池化   
      + 误差项回传时，传到上一层池化区域内最大值对应的神经元，其他神经元误差为0
  * 平均池化   
      + 误差项回传时，平均传到上一层池化区域内，所有神经元误差相同
- 卷积层
>![Eq 5.32](image/eq5.32.png)   
>![Eq 5.36](image/eq5.36.png)   

## 5. 几种典型的CNN
  - **LeNet-5**
>![LeNet5](image/LeNet-5_1.png)
>![LeNet5](image/LeNet-5_2.png)   

  - **AlexNet**
    * 首次采用GPU训练
    * 采用ReLU激活
    * 使用Dropout 防止过拟合
    >![AlexNet](image/AlexNet.png)
    >![AlexNet](image/AlexNet2.png)   
    注：图5.12中最左边224应该为227   
  - **Inception网络**
      * 由Inception 模块和少量池化层堆叠而成
      * GoogleNet 有9个Iception模块，5个池化层和其他卷积层和全连接层组成
      * Inception v3把大卷积核换成小卷积核
      * **1x1卷积核**的作用：
        + 当卷积核数量比输入的通道数少(多)时，能起到降维(升维)的作用  
        + 其降维或升维的本质是在通道间将信息线性组合变化，增加通道间的信息交互
        + 在保持feature map尺寸不变的情况下，大幅增加非线性特性，可以把网络做得更深
      >![Inception](image/Inception.png)   

  - **ResNet 残差网络**
      * 把目标函数拆分为恒等函数和残差函数，让神经网络去学习残差函数，比直接学习目标函数更容易
      >![ResNet](image/ResNet-1.png)   
      >![ResNet](image/ResNet-2.png)   

(第五章完结)

# 第六章 循环神经网络（RNN）
一类具有短期记忆能力的神经网络,长于处理时序数据并利用其历史信息。   

## 1. 给神经网络增加短期记忆能力的方法：
  - 延时神经网络（Time Delay NN）
    * 建立额外的延时单元，记录最近几次神经元的输出，使得下一层神经元的输入为上一层神经元最近多次的输出而不仅仅是一次。
  - 自回归模型(Auroregressive Model)
    * 用神经元的历史信息来预测自己，让历史信息能够影响现在
  - 循环神经网络（RNN）   

    >![RNN](image/RNN.png)   

## 2. 简单循环网络-只有一个隐藏层的神经网络
>![srn](image/srn-1.png)
>![srn](image/srn-2.png)

### 循环神经网络的计算能力
#### 通用近似定理
如果一个完全连接的循环神经网络有足够数量的sigmoid型隐藏神经元，它可以以任意准确率去近似任何一个非线性动力系统。
#### 图灵完备
所有的图灵机都可以被一个由使用Sigmoid 型激活函数的神经元构成的全连接循环网络来进行模拟。

#### 因此，一个完全连接的循环神经网络可以近似解决所有的可计算问题。

## 3. 应用到机器学习
### 序列到类别模式
输入序列，输出为类别，即分类模式，还可以按时间平均采样
>![s-c](image/s-c.png)
### 同步的序列到序列模式
主要用于序列标注任务,如词性标注，即每一时刻都有输入和输出，输入序列和输出序列的长度相同。
>![s-c](image/s-s.png)
### 异步的序列到序列模式
也称为编码器-解码器（Encoder-Decoder）模型，需要先编码后解码，
输入序列和输出序列不需要有严格的对应关系，也不需要保持相同的长度。
比如在机器翻译中，输入为源语言的单词序列，输出为目标语言的单词序列。
>![s-c](image/s-s-y.png)

## 4. 参数学习

（未完待续。。）
